---
title: "ESM 244 Lab 2"
author: "Heather Luedke"
date: "2023-01-19"
output: html_document
---

```{r setup, echo = TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(palmerpenguins) 
library(AICcmodavg)
library(equatiomatic)

```

# Predicting penguin mass 

```{r}
penguins_clean <- penguins %>% 
  drop_na() %>% 
  rename(mass = body_mass_g, bill_l = bill_length_mm, bill_d = bill_depth_mm, flip_l = flipper_length_mm)

mdl1 <- lm(mass ~ bill_l + bill_d + flip_l + species + sex + island, data = penguins_clean)
summary(mdl1) 
```

```{r}
f1 <- mass ~ bill_l + bill_d + flip_l + species + sex + island 
mdl1 <- lm(f1, data = penguins_clean) 

f2 <- mass ~ bill_l + bill_d + flip_l + species + sex 
mdl2 <- lm(f2, data = penguins_clean) 

f3 <- mass ~ bill_d + flip_l + species + sex 
mdl3 <- lm(f3, data = penguins_clean)

AIC(mdl1, mdl2, mdl3) 
BIC(mdl1, mdl2, mdl3) 

```

```{r}
AICcmodavg::AICc(mdl1) 

aictab(list(mdl1, mdl2, mdl3))

bictab(list(mdl1, mdl2, mdl3))
```

# Compare models using k-fold cross validation 

```{r}
folds <- 10 
fold_vec <- rep(1:folds, length.out = nrow(penguins_clean)) #make sure data set is randomized to bins 
#fold_vec to call the vector and see what's happening 

set.seed(42) 
#runif(1) 
penguins_fold <- penguins_clean %>% 
  mutate(group = sample(fold_vec, size = n(), replace = FALSE)) 

view(penguins_fold)

table(penguins_fold$group)
```

```{r}
test_df <- penguins_fold %>% 
  filter(group == 1)
train_df <- penguins_fold %>% 
  filter(group != 1)
```

```{r}
calc_mean <- function(x) {m <- sum(x) / length(x)}
calc_rmse <- function(x, y) 
{rmse <- (x-y)^2 %>% 
  mean() %>% 
  sqrt()
return(rmse)} #I'm going to give you an x and a y, subtract those, square it, take the average of those, the square root of it, and then return the final number 
```

```{r}
training_mdl1 <- lm(f1, data = train_df) #that's the 90% dataset 
training_mdl2 <- lm(f2, data = train_df) 
training_mdl3 <- lm(f3, data = train_df)

training_mdl1
training_mdl2
training_mdl3
```

```{r}
# could run AIC on those, but now . . . we're going to predict the species that it's never seen before and see how each does 

predict_test <- test_df %>% 
  mutate(model1 = predict(training_mdl1, test_df), 
         model2 = predict(training_mdl2, test_df), 
         model3 = predict(training_mdl3, test_df)) #put this into a big dataframe, so you can compare the actual mass to the mass each model predicted 

#view(predict_test) 

rmse_predict_test <- predict_test %>% 
  summarize(rmse_mdl1 = calc_rmse(model1, mass), 
            rmse_mdl2 = calc_rmse(model2, mass), 
            rmse_mdl3 = calc_rmse(model3, mass))#compares each model prediction and the actual mass of each penguin 

view(rmse_predict_test) #the smallest number is the preferred linear model to predict group 1. At this point we've only predicted through group 1, so we'd need to go back and do this for groups 2-10. That'd be annoying to redo all of this for the other groups. 
```

```{r}
rmse_df <- data.frame()
for(i in 1:folds) 
  ### i <- 1
  {kfold_test_df <- penguins_fold %>%  
  filter(group == i)
kfold_train_df <- penguins_fold %>% 
  filter(group != i)

kfold_mdl1 <- lm(f1, data = kfold_train_df)
kfold_mdl2 <- lm(f2, data = kfold_train_df)
kfold_mdl3 <- lm(f3, data = kfold_train_df)

# for this first instance here, we're going to assign it to a variable called i. for the next thing in our vector, we're going to take that new value and assign it to i 
#curly brackets open and close the loop 
#You can do this instead of doing it manually as above 

kfold_pred_df <- kfold_test_df %>% 
  mutate(mdl1 = predict(kfold_mdl1, .), 
         mdl2 = predict(kfold_mdl2, .), 
         mdl3 = predict(kfold_mdl3, .)) #use this model to predict the masses based on this data frame 
#the . indicates the dataframe we already have before the pipe operator 

kfold_rmse_df <- kfold_pred_df %>% 
  summarize(rmse_mdl1 = calc_rmse(mdl1, mass), 
            rmse_mdl2 = calc_rmse(mdl2, mass), 
            rmse_mdl3 = calc_rmse(mdl3, mass), 
            test_gp = i) 

rmse_df <- bind_rows(rmse_df, kfold_rmse_df) 
}

#which of these has the best predictive capability? 

```

```{r}

rmse_df %>% 
  summarize(mean_rmse_mdl1 = mean(rmse_mdl1), 
            mean_rmse_mdl2 = mean(rmse_mdl2), 
            mean_rmse_mdl3 = mean(rmse_mdl3)) 
#from whole for loop. so, take the average of all these rows to determine which on average has the best predictions 
#model 2 is a little lower than model 3 which is a little lower than model 1. 
```
## Finalize the model 

```{r}
final_mdl <- lm(f2, data = penguins_clean) 
```

our final model: 
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE)`

and with coefficients: 
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE, use_coefs = TRUE)`
